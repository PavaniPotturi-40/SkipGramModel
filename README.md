# SkipGramModel
# Skip-gram with Negative Sampling on Wikipedia

## ðŸ“Œ Overview
This project implements **Skip-gram with Negative Sampling (SGNS)** from scratch and trains it on a Wikipedia text dump.  
The learned word embeddings are compared with **Gensimâ€™s Word2Vec** model using cosine similarity.  
Further, the embeddings are evaluated using **word analogy tasks** and analyzed for **social bias**, as discussed in Chapter 5.

---

## ðŸ“š Dataset
- **Wikipedia dump (enwik8)**
- Source: https://mattmahoney.net/dc/textdata.html  
- Size: ~100MB cleaned Wikipedia text

---

## ðŸ§  Objectives
- Implement Skip-gram with Negative Sampling without using Gensim internally
- Train embeddings on Wikipedia text
- Compare custom embeddings with Gensim Word2Vec
- Perform word analogy evaluation
- Detect bias in word embeddings

---



